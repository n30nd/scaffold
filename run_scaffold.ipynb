{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK_DATASET = \"/media/namvq/Data/chest_xray\"\n",
    "# LINK_DATASET = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\n",
    "NUM_WORKERS = 6\n",
    "# BASE_FOLDER_NOISE = \"/kaggle/input/chest-xray-noise-60-partitions\"\n",
    "BASE_FOLDER_NOISE = \"/media/namvq/Data/code_chinh_sua\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/namvq/Data/anaconda3/envs/env_datn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKEND:  Agg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Partition the data and create the dataloaders.\"\"\"\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, Grayscale, ToTensor\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Chuyển sang backend không cần GUI\n",
    "\n",
    "print('BACKEND: ', matplotlib.get_backend())\n",
    "\n",
    "\n",
    "#/kaggle/input/chest-xray-pneumonia/chest_xray\n",
    "def get_custom_dataset(data_path: str = LINK_DATASET):\n",
    "    \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),  # Kích thước ảnh cho VGG\n",
    "        transforms.RandomAffine(degrees=0, shear=10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.2, 0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "                             [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Kích thước ảnh cho VGG\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "                             [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "    ])\n",
    "    trainset = ImageFolder(os.path.join(data_path, 'train'), transform=train_transform)\n",
    "    testset = ImageFolder(os.path.join(data_path, 'test'), transform=test_transform)\n",
    "    return trainset, testset\n",
    "#Lay tap val goc co 16 anh thoi\n",
    "def get_val_dataloader(batch_size: int = 10):\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    valset = ImageFolder(os.path.join(LINK_DATASET, 'val'), transform=val_transform)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    return valloader\n",
    "\n",
    "def prepare_dataset_for_centralized_train(batch_size: int, val_ratio: float = 0.1, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    # Split trainset into trainset and valset\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "\n",
    "def prepare_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1, alpha: float = 100, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate non-IID partitions using Dirichlet distribution.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    \n",
    "    # Split trainset into trainset and valset\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "    \n",
    "    # Generate Dirichlet distribution for each class\n",
    "    class_indices = [np.where(train_labels == i)[0] for i in range(len(np.unique(train_labels)))]\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    \n",
    "    for class_idx in class_indices:\n",
    "        np.random.shuffle(class_idx)\n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, num_partitions))\n",
    "        proportions = (np.cumsum(proportions) * len(class_idx)).astype(int)[:-1]\n",
    "        class_partitions = np.split(class_idx, proportions)\n",
    "        for i in range(num_partitions):\n",
    "            partition_indices[i].extend(class_partitions[i])\n",
    "    \n",
    "    # Create Subsets for each partition\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "    \n",
    "    # Split valset into partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "\n",
    "    valsets = random_split(valset, partition_len_val, torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Create DataLoaders for each partition\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Calculate class distribution for each partition in trainloaders\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_partitioned_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1, num_labels_each_party: int = 1, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate partitions where each party has a fixed number of labels.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()  # Load datasets\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels\n",
    "    num_labels = len(np.unique(train_labels))  # Assuming labels are 0 and 1 for binary classification\n",
    "    times = [0 for i in range(num_labels)]\n",
    "    contain = []\n",
    "    #Phan label cho cac client\n",
    "    for i in range(num_partitions):\n",
    "        current = [i%num_labels]\n",
    "        times[i%num_labels] += 1\n",
    "        if num_labels_each_party > 1:\n",
    "            current.append(1-i%num_labels)\n",
    "            times[1-i%num_labels] += 1\n",
    "        contain.append(current)\n",
    "    print(times)\n",
    "    print(contain)\n",
    "    # Create Subsets for each partition\n",
    "\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    for i in range(num_labels):\n",
    "        idx_i = np.where(train_labels == i)[0]  # Get indices of label i in train_labels\n",
    "        idx_i = [trainset.indices[j] for j in idx_i]  # Convert indices to indices in trainset\n",
    "        # #print label of idx_i\n",
    "        # print(\"Label of idx: \", i)\n",
    "        # for j in range(len(idx_i)):\n",
    "        #     idx_in_dataset = trainset.indices[idx_i[j]]\n",
    "        #     print(trainset.dataset.targets[idx_in_dataset])\n",
    "        np.random.shuffle(idx_i)\n",
    "        split = np.array_split(idx_i, times[i])\n",
    "        ids = 0\n",
    "        for j in range(num_partitions):\n",
    "            if i in contain[j]:\n",
    "                partition_indices[j].extend(split[ids])\n",
    "                ids += 1\n",
    "    \n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    # #print label of client 0\n",
    "    # print(\"Client 0\")\n",
    "    # for i in range(len(trainsets[0])):\n",
    "    #     print(trainsets[0][i][1])\n",
    "\n",
    "    # Split valset into partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Create DataLoaders for each partition\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Calculate class distribution for each partition in trainloaders\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_imbalance_label_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 0.5, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate partitions where each party has a fixed number of labels.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()  # Load datasets\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels\n",
    "    num_labels = len(np.unique(train_labels))  # Assuming labels are 0 and 1 for binary classification\n",
    "    min_size = 0\n",
    "    min_require_size = 2\n",
    "\n",
    "    N = len(trainset)\n",
    "\n",
    "\n",
    "    while(min_size < min_require_size):\n",
    "        partition_indices = [[] for _ in range(num_partitions)]\n",
    "        for label in range(num_labels):\n",
    "            idx_label = np.where(train_labels == label)[0]\n",
    "            idx_label = [trainset.indices[j] for j in idx_label]\n",
    "            np.random.shuffle(idx_label)\n",
    "\n",
    "            proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "            # proportions = np.array( [p * len(idx_j) < N/num_partitions] for p, idx_j in zip(proportions, partition_indices))\n",
    "            proportions = np.array([p if p * len(idx_j) < N / num_partitions else 0 for p, idx_j in zip(proportions, partition_indices)])\n",
    "\n",
    "            proportions = proportions / np.sum(proportions)\n",
    "            proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "\n",
    "            partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, np.split(idx_label, proportions))]\n",
    "            min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "        \n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "\n",
    "def apply_gaussian_noise(tensor, std_dev):\n",
    "    noise = torch.randn_like(tensor) * std_dev\n",
    "    return tensor + noise\n",
    "\n",
    "# Hàm đảo ngược chuẩn hóa\n",
    "def unnormalize_image(image_tensor, mean, std):\n",
    "    # Đảo ngược Normalize: (image * std) + mean\n",
    "    for t, m, s in zip(image_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Thực hiện từng kênh\n",
    "    return image_tensor\n",
    "\n",
    "# Hàm hiển thị ảnh từ một tensor\n",
    "def display_image(image_tensor, mean, std):\n",
    "    # Đảo ngược chuẩn hóa\n",
    "    image_tensor = unnormalize_image(image_tensor, mean, std)\n",
    "    # Chuyển tensor thành NumPy array và điều chỉnh thứ tự kênh màu (CHW -> HWC)\n",
    "    image_numpy = image_tensor.permute(1, 2, 0).numpy()\n",
    "    # Cắt giá trị ảnh về phạm vi [0, 1] để hiển thị đúng\n",
    "    image_numpy = image_numpy.clip(0, 1)\n",
    "    # Trả về ảnh NumPy\n",
    "    return image_numpy\n",
    "\n",
    "# def prepare_noise_based_imbalance(num_partitions: int, batch_size: int, val_ratio: float = 0.1, sigma: float = 0.05, seed: int = 42):\n",
    "#     \"\"\"\n",
    "#     Chia du lieu ngau nhien va deu cho cac ben, sau do them noise vao cac ben\n",
    "#     moi ben i co noise khac nhau Gauss(0, sigma*i/N)\n",
    "#     \"\"\"\n",
    "#     trainset, testset = get_custom_dataset()\n",
    "#     num_train = int((1 - val_ratio) * len(trainset))\n",
    "#     num_val = len(trainset) - num_train\n",
    "#     trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "#     indices = trainset.indices\n",
    "\n",
    "#     np.random.shuffle(indices)\n",
    "\n",
    "#     partition_indices = np.array_split(indices, num_partitions)\n",
    "\n",
    "#     train_partitions = []\n",
    "\n",
    "#     for i, part_indices in enumerate(partition_indices):\n",
    "#         partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "#         partition_set = Subset(trainset.dataset, part_indices)\n",
    "        \n",
    "#         noisy_samples = [apply_gaussian_noise(sample[0], partition_std_dev) for sample in partition_set]\n",
    "#         noisy_dataset = [(noisy_samples[j], trainset.dataset[part_indices[j]][1]) for j in range(len(part_indices))]\n",
    "#         # train_partitions.append((noisy_samples, [sample[1] for sample in partition_set]))\n",
    "#         train_partitions.append(noisy_dataset)\n",
    "#     trainloaders = [DataLoader(train_partitions[i], batch_size=batch_size, shuffle=True, num_workers=4) for i in range(num_partitions)]\n",
    "#     partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "#     for i in range(len(valset) % num_partitions):\n",
    "#         partition_len_val[i] += 1\n",
    "    \n",
    "#     valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "#     valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "#     testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# ####\n",
    "#     class_distributions = []\n",
    "#     for i, trainloader in enumerate(trainloaders):\n",
    "#         class_counts = Counter()\n",
    "#         for _, labels in trainloader:\n",
    "#             class_counts.update(labels.numpy())\n",
    "#         class_distributions.append(class_counts)\n",
    "#         print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "#     partitions = range(num_partitions)\n",
    "#     class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "#     class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "#     bar_width = 0.5\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "#     plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "#     plt.xlabel('Partition')\n",
    "#     plt.ylabel('Number of Samples')\n",
    "#     plt.title('Class Distribution in Each Partition')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     # plt.show()\n",
    "#     #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "#     output_dir = 'running_outputs'\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "#     plt.close()\n",
    "\n",
    "#     #Lưu ảnh nhiễu vào running_outputs\n",
    "#     # Mean và std từ Normalize\n",
    "#     mean = [0.485, 0.456, 0.406]\n",
    "#     std = [0.229, 0.224, 0.225]\n",
    "\n",
    "#     # Tạo thư mục lưu ảnh nếu chưa tồn tại\n",
    "#     output_dir = \"running_outputs\"\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Khởi tạo một lưới 10x6 để hiển thị ảnh\n",
    "#     fig, axes = plt.subplots(10, 6, figsize=(15, 25))\n",
    "\n",
    "#     # Duyệt qua 60 trainloaders và hiển thị ảnh đầu tiên\n",
    "#     for i, trainloader in enumerate(trainloaders[:num_partitions]):\n",
    "#         # Lấy ảnh đầu tiên từ trainloader\n",
    "#         image_tensor = trainloader.dataset[0][0].clone()  # Clone để tránh thay đổi dữ liệu gốc\n",
    "        \n",
    "#         # Tìm vị trí hàng, cột trong lưới\n",
    "#         row, col = divmod(i, 6)\n",
    "#         plt.sca(axes[row, col])  # Đặt trục hiện tại là vị trí hàng, cột trong lưới\n",
    "        \n",
    "#         # Hiển thị ảnh\n",
    "#         image_numpy = display_image(image_tensor, mean, std)\n",
    "#         axes[row, col].imshow(image_numpy)\n",
    "#         axes[row, col].axis('off')\n",
    "#     plt.title(f\"Noise image with sigma from {sigma * 1 / num_partitions} to {sigma}\")\n",
    "#     # Điều chỉnh layout để không bị chồng lấn\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     # Lưu ảnh thay vì hiển thị\n",
    "#     output_path = os.path.join(output_dir, \"image_noise.png\")\n",
    "#     plt.savefig(output_path, dpi=300)  # Lưu ảnh với chất lượng cao\n",
    "\n",
    "#     plt.close()  # Đóng figure\n",
    "\n",
    "#     print(f\"Ảnh đã được lưu tại {output_path}\")\n",
    "\n",
    "#     print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "# ###\n",
    "#     return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_noise_based_imbalance(num_partitions: int, batch_size: int, val_ratio: float = 0.1, sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Chia dữ liệu ngẫu nhiên và đều cho các bên, sau đó thêm noise vào các bên.\n",
    "    Mỗi bên i có noise khác nhau Gauss(0, sigma*i/N). Nếu dữ liệu đã tồn tại, tải từ thư mục dataset_noise_{sigma}.\n",
    "    \"\"\"\n",
    "    # noise_dir = f'chest_xray_noise_{sigma}'\n",
    "    # noise_dir = f'/kaggle/input/chest-xray-noise-60-partitions/chest_xray_noise_{sigma}'\n",
    "    noise_dir = f\"{BASE_FOLDER_NOISE}/chest_xray_noise_{sigma}\"\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    noisy_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    if os.path.exists(noise_dir):\n",
    "        print(f\"Loading noisy dataset from {noise_dir}...\")\n",
    "        # Sử dụng ImageFolder để tải dữ liệu đã được thêm nhiễu với transform phù hợp\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) for i in range(num_partitions)]\n",
    "        \n",
    "        # Tải val và test set như bình thường\n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        _, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Chia valset thành các partition\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Tạo DataLoaders\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) for part in train_partitions]\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "        print(\"Dữ liệu đã được tải thành công từ thư mục lưu trữ.\")\n",
    "    else:\n",
    "        print(f\"Creating noisy dataset and saving to {noise_dir}...\")\n",
    "        os.makedirs(noise_dir, exist_ok=True)\n",
    "        \n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "    \n",
    "        indices = trainset.indices\n",
    "        np.random.shuffle(indices)\n",
    "        partition_indices = np.array_split(indices, num_partitions)\n",
    "    \n",
    "        # Mean và std từ Normalize đã được định nghĩa trước\n",
    "        for i, part_indices in enumerate(partition_indices):\n",
    "            partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "            partition_set = Subset(trainset.dataset, part_indices)\n",
    "            \n",
    "            # Tạo thư mục cho partition và các lớp\n",
    "            partition_dir = os.path.join(noise_dir, f'partition_{i}')\n",
    "            os.makedirs(partition_dir, exist_ok=True)\n",
    "            class_dirs = {}\n",
    "            for _, label in partition_set:\n",
    "                if label not in class_dirs:\n",
    "                    class_dirs[label] = os.path.join(partition_dir, f'class_{label}')\n",
    "                    os.makedirs(class_dirs[label], exist_ok=True)\n",
    "            \n",
    "            for j, (image, label) in enumerate(partition_set):\n",
    "                noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "                # Đảo ngược chuẩn hóa để lưu ảnh đúng định dạng\n",
    "                noisy_image = unnormalize_image(noisy_image, mean, std)\n",
    "                # Chuyển tensor thành PIL Image\n",
    "                noisy_image_pil = transforms.ToPILImage()(noisy_image.clamp(0, 1))\n",
    "                # Lưu ảnh với tên duy nhất\n",
    "                image_filename = f'image_{j}.png'\n",
    "                noisy_image_pil.save(os.path.join(class_dirs[label], image_filename))\n",
    "        \n",
    "        # Tải dữ liệu từ thư mục đã lưu với transform phù hợp\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) for i in range(num_partitions)]\n",
    "        \n",
    "        # Chia valset thành các partition\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Tạo DataLoaders\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) for part in train_partitions]\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "    \n",
    "    # Phân tích phân bố lớp\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i].get(0, 0) for i in partitions]\n",
    "    class_1_counts = [class_distributions[i].get(1, 0) for i in partitions]\n",
    "    \n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Lưu ảnh nhiễu vào running_outputs\n",
    "    # Tạo thư mục lưu ảnh nếu chưa tồn tại\n",
    "    output_dir = \"running_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Khởi tạo một lưới 10x6 để hiển thị ảnh\n",
    "    fig, axes = plt.subplots(10, 6, figsize=(15, 25))\n",
    "    \n",
    "    # Duyệt qua trainloaders và hiển thị ảnh đầu tiên từ mỗi partition\n",
    "    for i, trainloader in enumerate(trainloaders[:min(num_partitions, 60)]):\n",
    "        if len(trainloader.dataset) == 0:\n",
    "            continue\n",
    "        # Lấy ảnh đầu tiên từ trainloader\n",
    "        image_tensor, label = trainloader.dataset[0]\n",
    "        \n",
    "        # Tìm vị trí hàng, cột trong lưới\n",
    "        row, col = divmod(i, 6)\n",
    "        if row >= 10:\n",
    "            break  # Chỉ hiển thị tối đa 60 ảnh\n",
    "        # Hiển thị ảnh\n",
    "        image_numpy = unnormalize_image(image_tensor.clone(), mean, std).permute(1, 2, 0).numpy().clip(0, 1)\n",
    "        axes[row, col].imshow(image_numpy)\n",
    "        axes[row, col].axis('off')\n",
    "    # Điều chỉnh layout để không bị chồng lấn\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Lưu ảnh thay vì hiển thị\n",
    "    output_path = os.path.join(output_dir, \"image_noise.png\")\n",
    "    plt.savefig(output_path, dpi=300)  # Lưu ảnh với chất lượng cao\n",
    "    \n",
    "    plt.close()  # Đóng figure\n",
    "    \n",
    "    print(f\"Ảnh minh họa đã được lưu tại {output_path}\")\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    \n",
    "    return trainloaders, valloaders, testloader\n",
    "def prepare_quantity_skew_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 10, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    all_indices = trainset.indices\n",
    "\n",
    "    min_size = 0\n",
    "    while min_size < 1:\n",
    "        proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "        proportions = (np.cumsum(proportions) * len(all_indices)).astype(int)[:-1]\n",
    "\n",
    "        partition_indices = np.split(all_indices, proportions)\n",
    "\n",
    "        min_size = min([len(partition) for partition in partition_indices])\n",
    "        print('Partition sizes:', [len(partition) for partition in partition_indices])\n",
    "        print('Min partition size:', min_size)\n",
    "\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_label_drl_and_noisy_data(num_partitions: int, batch_size: int, \n",
    "                                      val_ratio: float = 0.1, beta: float = 0.5, \n",
    "                                      sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Phân chia dữ liệu với phân phối không cân bằng và thêm nhiễu Gaussian.\n",
    "    Nếu dữ liệu đã được lưu, tải từ thư mục lưu trữ; nếu chưa, thực hiện phân chia và lưu.\n",
    "    \"\"\"\n",
    "    # Định nghĩa thư mục lưu trữ dựa trên các tham số\n",
    "    # noise_dir = f\"data_partition_combined_{num_partitions}_beta_{beta}_sigma_{sigma}\"\n",
    "    noise_dir = f\"{BASE_FOLDER_NOISE}/chest_xray_noise_drl_label{beta}_{sigma}\"\n",
    "\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    noisy_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    if os.path.exists(noise_dir):\n",
    "        print(f\"Loading partitioned and noisy dataset from {noise_dir}...\")\n",
    "        # Tải các partition đã lưu bằng ImageFolder\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "                            for i in range(num_partitions)]\n",
    "        \n",
    "        # Lấy số lượng lớp từ một trong các partition\n",
    "        if len(train_partitions) > 0:\n",
    "            num_labels = len(train_partitions[0].classes)\n",
    "        else:\n",
    "            raise ValueError(\"Không tìm thấy partition nào trong thư mục lưu trữ.\")\n",
    "        \n",
    "        # Tải val và test set như bình thường\n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        _, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Chia valset thành các partition\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Tạo DataLoaders\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "                       for part in train_partitions]\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) \n",
    "                      for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "        print(\"Dữ liệu đã được tải thành công từ thư mục lưu trữ.\")\n",
    "    else:\n",
    "        print(f\"Creating partitioned and noisy dataset and saving to {noise_dir}...\")\n",
    "        os.makedirs(noise_dir, exist_ok=True)\n",
    "        \n",
    "        trainset, testset = get_custom_dataset()\n",
    "        num_train = int((1 - val_ratio) * len(trainset))\n",
    "        num_val = len(trainset) - num_train\n",
    "        train_subset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Lấy nhãn của toàn bộ trainset\n",
    "        train_labels = np.array([train_subset.dataset.targets[i] for i in train_subset.indices])\n",
    "        num_labels = len(np.unique(train_labels))\n",
    "        # min_size = 0\n",
    "        # min_require_size = 2\n",
    "        # N = len(train_subset)\n",
    "        \n",
    "        # # Phân chia dữ liệu theo phân phối Dirichlet\n",
    "        # while min_size < min_require_size:\n",
    "        #     partition_indices = [[] for _ in range(num_partitions)]\n",
    "        #     for label in range(num_labels):\n",
    "        #         idx_label = np.where(train_labels == label)[0]\n",
    "        #         idx_label = [train_subset.indices[j] for j in idx_label]\n",
    "        #         np.random.shuffle(idx_label)\n",
    "                \n",
    "        #         proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "        #         proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "                \n",
    "        #         splits = np.split(idx_label, proportions)\n",
    "        #         partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, splits)]\n",
    "        #     min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "        \n",
    "        min_size = 0\n",
    "        N = len(train_subset)\n",
    "\n",
    "        # Phân chia dữ liệu theo phân phối Dirichlet\n",
    "        while min_size < 1:\n",
    "            partition_indices = [[] for _ in range(num_partitions)]\n",
    "            for label in range(num_labels):\n",
    "                idx_label = np.where(train_labels == label)[0]\n",
    "                idx_label = [train_subset.indices[j] for j in idx_label]\n",
    "                np.random.shuffle(idx_label)\n",
    "\n",
    "                if len(idx_label) < num_partitions:\n",
    "                    raise ValueError(f\"Không đủ mẫu lớp {label} để phân phối cho tất cả partitions.\")\n",
    "\n",
    "                # Gán một mẫu cho mỗi partition\n",
    "                for i in range(num_partitions):\n",
    "                    partition_indices[i].append(idx_label[i])\n",
    "\n",
    "                # Phân phối các mẫu còn lại theo Dirichlet\n",
    "                remaining_idx = idx_label[num_partitions:]\n",
    "                if len(remaining_idx) > 0:\n",
    "                    proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "                    proportions = (proportions * len(remaining_idx)).astype(int)\n",
    "\n",
    "                    # Điều chỉnh proportions để đảm bảo tổng đúng bằng số mẫu còn lại\n",
    "                    while proportions.sum() < len(remaining_idx):\n",
    "                        proportions[np.argmax(proportions)] += 1\n",
    "                    while proportions.sum() > len(remaining_idx):\n",
    "                        proportions[np.argmax(proportions)] -= 1\n",
    "\n",
    "                    splits = np.split(remaining_idx, np.cumsum(proportions)[:-1])\n",
    "                    for i, idx in enumerate(splits):\n",
    "                        partition_indices[i].extend(idx.tolist())\n",
    "\n",
    "            min_sizes = [len(set(train_labels[[idx for idx in part]])) for part in partition_indices]\n",
    "            min_size = min(min_sizes)\n",
    "        # Lưu các partition với nhiễu Gaussian\n",
    "        for i, indices in enumerate(partition_indices):\n",
    "            partition_dir = os.path.join(noise_dir, f'partition_{i}')\n",
    "            os.makedirs(partition_dir, exist_ok=True)\n",
    "            for label in range(num_labels):\n",
    "                class_dir = os.path.join(partition_dir, f'class_{label}')\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "            class_dirs = {}\n",
    "            for idx in indices:\n",
    "                _, label = train_subset.dataset[idx]\n",
    "                if label not in class_dirs:\n",
    "                    class_dirs[label] = os.path.join(partition_dir, f'class_{label}')\n",
    "                    os.makedirs(class_dirs[label], exist_ok=True)\n",
    "            \n",
    "            # Thêm nhiễu và lưu ảnh\n",
    "            partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "            for j, idx in enumerate(indices):\n",
    "                image, label = train_subset.dataset[idx]\n",
    "                noisy_image = apply_gaussian_noise(image, partition_std_dev)\n",
    "                noisy_image = unnormalize_image(noisy_image, mean, std)\n",
    "                noisy_image_pil = transforms.ToPILImage()(noisy_image.clamp(0, 1))\n",
    "                image_filename = f'image_{j}.png'\n",
    "                noisy_image_pil.save(os.path.join(class_dirs[label], image_filename))\n",
    "        \n",
    "        # Tải các partition đã lưu bằng ImageFolder\n",
    "        train_partitions = [ImageFolder(os.path.join(noise_dir, f'partition_{i}'), transform=noisy_transform) \n",
    "                            for i in range(num_partitions)]\n",
    "        \n",
    "        # Chia valset thành các partition\n",
    "        partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "        for i in range(len(valset) % num_partitions):\n",
    "            partition_len_val[i] += 1\n",
    "        valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "        \n",
    "        # Tạo DataLoaders\n",
    "        trainloaders = [DataLoader(part, batch_size=batch_size, shuffle=True, num_workers=6) \n",
    "                       for part in train_partitions]\n",
    "        valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=6) \n",
    "                      for vs in valsets]\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "        \n",
    "        print(f\"Dữ liệu đã được phân chia và lưu tại {noise_dir}\")\n",
    "    \n",
    "    # Phân tích phân bố lớp\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    # Vẽ biểu đồ phân bố lớp\n",
    "    partitions = range(num_partitions)\n",
    "    class_counts_list = []\n",
    "    for i in partitions:\n",
    "        counts = {cls: class_distributions[i].get(cls, 0) for cls in range(num_labels)}\n",
    "        class_counts_list.append(counts)\n",
    "    \n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bottom = np.zeros(num_partitions)\n",
    "    colors = plt.cm.tab10.colors  # Sử dụng bảng màu có sẵn\n",
    "    \n",
    "    for cls in range(num_labels):\n",
    "        counts = [class_counts_list[i].get(cls, 0) for i in partitions]\n",
    "        plt.bar(partitions, counts, bar_width, bottom=bottom, label=f'Class {cls}', color=colors[cls % len(colors)])\n",
    "        bottom += counts\n",
    "    \n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition_combined.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f'Number of train samples: {sum(len(loader.dataset) for loader in trainloaders)}, '\n",
    "          f'val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    \n",
    "    return trainloaders, valloaders, testloader\n",
    "    \n",
    "def load_datasets(\n",
    "    config: DictConfig,\n",
    "    num_clients: int,\n",
    "    val_ratio: float = 0.1,\n",
    "    seed: Optional[int] = 42,\n",
    ") -> Tuple[List[DataLoader], List[DataLoader], DataLoader]:\n",
    "    \"\"\"Create the dataloaders to be fed into the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config: DictConfig\n",
    "        Parameterises the dataset partitioning process\n",
    "    num_clients : int\n",
    "        The number of clients that hold a part of the data\n",
    "    val_ratio : float, optional\n",
    "        The ratio of training data that will be used for validation (between 0 and 1),\n",
    "        by default 0.1\n",
    "    seed : int, optional\n",
    "        Used to set a fix seed to replicate experiments, by default 42\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[DataLoader, DataLoader, DataLoader]\n",
    "        The DataLoaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    print(f\"Dataset partitioning config: {config}\")\n",
    "    batch_size = -1\n",
    "    print('config:' , config)\n",
    "    if \"batch_size\" in config:\n",
    "        batch_size = config.batch_size\n",
    "    elif \"batch_size_ratio\" in config:\n",
    "        batch_size_ratio = config.batch_size_ratio\n",
    "    else:\n",
    "        raise ValueError\n",
    "    partitioning = \"\"\n",
    "    \n",
    "    if \"partitioning\" in config:\n",
    "        partitioning = config.partitioning\n",
    "\n",
    "    # partition the data\n",
    "    if partitioning == \"imbalance_label\":\n",
    "        return prepare_partitioned_dataset(num_clients, batch_size, val_ratio, config.labels_per_client, config.seed)\n",
    "\n",
    "    if partitioning == \"imbalance_label_dirichlet\":\n",
    "        return prepare_imbalance_label_dirichlet(num_clients, batch_size, val_ratio, config.alpha, config.seed)\n",
    "\n",
    "    if partitioning == \"noise_based_imbalance\":\n",
    "        return prepare_noise_based_imbalance(num_clients, batch_size, val_ratio, config.sigma, config.seed)\n",
    "\n",
    "    if partitioning == \"quantity_skew_dirichlet\":\n",
    "        return prepare_quantity_skew_dirichlet(num_clients, batch_size, val_ratio, config.alpha, config.seed)\n",
    "    \n",
    "    if partitioning == \"label_drl_and_noisy\":\n",
    "        return prepare_label_drl_and_noisy_data(num_clients, batch_size, \n",
    "                                      val_ratio, config.alpha, \n",
    "                                      config.sigma, config.seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        # self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)  # Output corresponds to num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class VGG11Model(nn.Module):\n",
    "    # Implement VGG11 model for transfer learning\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = models.vgg11(pretrained=True)\n",
    "        \n",
    "        # Freeze the convolutional base\n",
    "        # for param in self.model.features.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        # Replace avgpool with AdaptiveAvgPool2d\n",
    "        self.model.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Replace the classifier with a new one\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)  # Output corresponds to num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim \n",
    "import copy\n",
    "import random \n",
    "import numpy as np\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def reset_model_to_zero(model):\n",
    "    for param in model.parameters():\n",
    "        param.data.fill_(0.0)\n",
    "\n",
    "def federated_train(trainloaders, valloaders, testloader, config):\n",
    "    # model = ResNet18(num_classes=2)\n",
    "    model = VGG11Model(num_classes=2)\n",
    "    nets = {net_i: copy.deepcopy(model) for net_i in range(len(trainloaders))}\n",
    "    global_model = copy.deepcopy(model)  # Bản sao mô hình toàn cục\n",
    "    c_nets = {net_i: copy.deepcopy(model) for net_i in range(len(trainloaders))}  # Bản sao mô hình trên từng client\n",
    "    c_global = copy.deepcopy(model)\n",
    "\n",
    "    # c_global_para = c_global.state_dict()\n",
    "    # for net_id, net in c_nets.items():\n",
    "    #     net.load_state_dict(c_global_para)\n",
    "    for c_net in c_nets.values():\n",
    "        reset_model_to_zero(c_net)\n",
    "    reset_model_to_zero(c_global)\n",
    "\n",
    "    valloader_goc = get_val_dataloader()\n",
    "    num_rounds = config.num_rounds  # Số vòng huấn luyện\n",
    "    accs_test = []\n",
    "    accs_val = []\n",
    "    accs_test.append(evaluate(global_model, testloader))\n",
    "    accs_val.append(evaluate(global_model, valloader_goc))\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Round {round_num + 1}/{num_rounds}\")\n",
    "        start = time.time()\n",
    "        global_para = global_model.state_dict()\n",
    "\n",
    "        # Chọn các client tham gia vào mỗi round\n",
    "        selected_clients = select_clients(trainloaders, config.clients_per_round)\n",
    "        \n",
    "        # Huấn luyện trên các client đã chọn\n",
    "        for client in selected_clients:\n",
    "            nets[client].load_state_dict(global_para)\n",
    "        local_train_net_scaffold(nets, selected_clients, global_model, c_nets, c_global, config, trainloaders, device=DEVICE)\n",
    "\n",
    "        total_data_points = sum([len(trainloaders[client].dataset) for client in selected_clients])\n",
    "        freqs = [len(trainloaders[client].dataset) / total_data_points for client in selected_clients]\n",
    "\n",
    "        for idx in range(len(selected_clients)):\n",
    "            net_para = nets[selected_clients[idx]].cpu().state_dict()\n",
    "            if idx == 0:\n",
    "                for key in net_para:\n",
    "                    global_para[key] = net_para[key] * freqs[idx]\n",
    "            else:\n",
    "                for key in net_para:\n",
    "                    global_para[key] += net_para[key] * freqs[idx]\n",
    "        global_model.load_state_dict(global_para)\n",
    "        global_model.to('cpu')\n",
    "        acc_test = evaluate(global_model, testloader)        \n",
    "        acc_val = evaluate(global_model, valloader_goc)\n",
    "\n",
    "        accs_test.append(acc_test)\n",
    "        accs_val.append(acc_val)\n",
    "        print('Acc_val: ', acc_val)\n",
    "        print('Acc_test: ', acc_test)\n",
    "        # if round_num >= 0:\n",
    "        #     if acc_val > 80.0:\n",
    "        #         config.learning_rate = 1e-8\n",
    "        #         print(f\"Accuracy > 80%, decreasing learning rate to {config.learning_rate}\")\n",
    "        #     elif acc_val > 70.0:\n",
    "        #         config.learning_rate = 1e-7\n",
    "        #         print(f\"Accuracy > 70%, decreasing learning rate to {config.learning_rate}\")\n",
    "        #     elif acc_val > 60.0:\n",
    "        #         config.learning_rate = 1e-6\n",
    "        #         print(f\"Accuracy > 60%, decreasing learning rate to {config.learning_rate}\")\n",
    "        #     elif acc_val > 50.0:\n",
    "        #         config.learning_rate = 1e-5\n",
    "        #         print(f\"Accuracy > 50%, decreasing learning rate to {config.learning_rate}\")\n",
    "        #     else :\n",
    "        #         config.learning_rate = 1e-4\n",
    "        #         print(f\"Accuracy <= 50%, increasing learning rate to {config.learning_rate}\")\n",
    "\n",
    "        end = time.time()\n",
    "        print(f'Time for round {round_num + 1}: ', end-start)\n",
    "    # plot_accuracy(accs)\n",
    "        # plot_accuracy(accs)\n",
    "    print('accuracies test: ', accs_test)\n",
    "    print('accuracies val: ', accs_val)\n",
    "    plt.plot(range(0, num_rounds + 1), accs_test, marker='o', label='Accuracy_test')\n",
    "    plt.plot(range(0, num_rounds + 1), accs_val, marker='x', label='Accuracy_val')\n",
    "    plt.xlabel('Round')\n",
    "    plt.xticks(range(0, num_rounds + 1, 10))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('WOW WOW WOW')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    idx_file = 1\n",
    "    new_file_name = f'running_outputs/accuracy_summary_{idx_file}.png'\n",
    "    while os.path.exists(f'running_outputs/accuracy_summary_{idx_file}.png'):\n",
    "        idx_file += 1\n",
    "        new_file_name = f'running_outputs/accuracy_summary_{idx_file}.png'\n",
    "    \n",
    "    plt.savefig(new_file_name)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def local_train_net_scaffold(nets, selected_clients, global_model, c_nets, c_global, config, trainloaders, device='cpu'):\n",
    "    total_delta = copy.deepcopy(global_model.state_dict())\n",
    "    for key in total_delta:\n",
    "        total_delta[key] = 0.0\n",
    "    c_global.to(device)\n",
    "    global_model.to(device)\n",
    "    for net_id in selected_clients:\n",
    "        net = nets[net_id]\n",
    "        net.to(device)\n",
    "\n",
    "        c_nets[net_id].to(device)\n",
    "\n",
    "        c_delta_para = train_net_scaffold(net, global_model, c_nets[net_id], c_global, trainloaders[net_id], config, device=device)\n",
    "        c_nets[net_id].to('cpu')\n",
    "        for key in total_delta:\n",
    "            total_delta[key] += c_delta_para[key]\n",
    "        \n",
    "    for key in total_delta:\n",
    "        # total_delta[key] /= len(selected_clients) ### ???\n",
    "        total_delta[key] /= config.num_clients\n",
    "    c_global_para = c_global.state_dict()\n",
    "    for key in c_global_para:\n",
    "        if c_global_para[key].type() == 'torch.LongTensor':\n",
    "            c_global_para[key] += total_delta[key].type(torch.LongTensor)\n",
    "        elif c_global_para[key].type() == 'torch.cuda.LongTensor':\n",
    "            c_global_para[key] += total_delta[key].type(torch.cuda.LongTensor)\n",
    "        else:\n",
    "            #print(c_global_para[key].type())\n",
    "            c_global_para[key] += total_delta[key]\n",
    "    c_global.load_state_dict(c_global_para)\n",
    "\n",
    "    # nets_list = list(nets.values())\n",
    "    # return nets_list\n",
    "def train_net_scaffold(net, global_model, c_local, c_global, trainloader, config, device):\n",
    "    optimizer = optim.SGD(\n",
    "        filter(lambda p: p.requires_grad, net.parameters()),\n",
    "        lr=config.learning_rate,\n",
    "        momentum=config.momentum\n",
    "    )\n",
    "    critierion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    c_local.to(device)\n",
    "    c_global.to(device)\n",
    "    global_model.to(device)\n",
    "\n",
    "    c_global_para = c_global.state_dict()\n",
    "    c_local_para = c_local.state_dict()\n",
    "    cnt = 0\n",
    "    for _ in range(config.num_epochs):\n",
    "        for data, target in trainloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = critierion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            net_para = net.state_dict()\n",
    "            for key in net_para:\n",
    "                net_para[key] = net_para[key] - config.learning_rate * (c_global_para[key] - c_local_para[key])\n",
    "            net.load_state_dict(net_para)\n",
    "            cnt += 1\n",
    "    \n",
    "    c_new_para = c_local.state_dict()\n",
    "    c_delta_para = copy.deepcopy(c_local.state_dict())\n",
    "    global_model_para = global_model.state_dict()\n",
    "    net_para = net.state_dict()\n",
    "    for key in net_para:\n",
    "        c_new_para[key] = c_new_para[key] - c_global_para[key] + (global_model_para[key] - net_para[key]) / (cnt * config.learning_rate)\n",
    "        c_delta_para[key] = c_new_para[key] - c_local_para[key]\n",
    "    c_local.load_state_dict(c_new_para)\n",
    "\n",
    "    net.to('cpu')\n",
    "    return c_delta_para\n",
    "\n",
    "            \n",
    "        \n",
    "def plot_accuracy(accs):\n",
    "    print('accuracies: ', accs)\n",
    "    num_rounds = len(accs)-1\n",
    "    plt.plot(range(0, num_rounds + 1), accs, marker='o', label='Accuracy')\n",
    "    plt.xlabel('Round')\n",
    "    plt.xticks(range(0, num_rounds + 1))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Scaffold on ResNet18 over Rounds')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig('running_outputs/accuracy_summary.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def select_clients(trainloaders, clients_per_round):\n",
    "    \"\"\"Chọn ngẫu nhiên một số client tham gia huấn luyện trong mỗi round.\"\"\"\n",
    "    # Số lượng client có sẵn\n",
    "    total_clients = len(trainloaders)\n",
    "    # Chọn ngẫu nhiên một số client\n",
    "    selected_clients = random.sample(range(total_clients), clients_per_round)\n",
    "    return selected_clients\n",
    "\n",
    "\n",
    "def evaluate(model, testloader):\n",
    "    \"\"\"Đánh giá mô hình trên tập kiểm tra.\"\"\"\n",
    "    print('evaluate on', DEVICE)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()  # Chuyển sang chế độ đánh giá\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    model.to('cpu')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘running_outputs’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir running_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đường dẫn thư mục\n",
    "directory = 'running_outputs/'\n",
    "\n",
    "# Xóa tất cả tệp trong thư mục bằng một lệnh duy nhất\n",
    "for file in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Config:\n",
      "{'num_clients': 60, 'num_epochs': 1, 'batch_size': 10, 'clients_per_round': 2, 'fraction_fit': 0.1, 'learning_rate': 0.001, 'num_rounds': 1, 'partitioning': 'label_drl_and_noisy', 'dataset_name': 'chest_xray', 'dataset_seed': 42, 'alpha': 0.5, 'sigma': 0.1, 'labels_per_client': 1, 'momentum': 0.9, 'weight_decay': 1e-05, 'dataset': {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}}\n",
      "Dataset partitioning config: {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}\n",
      "config: {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}\n",
      "Loading partitioned and noisy dataset from /media/namvq/Data/code_chinh_sua/chest_xray_noise_drl_label0.5_0.1...\n",
      "Dữ liệu đã được tải thành công từ thư mục lưu trữ.\n",
      "Partition 0 class distribution: {1: 34, 0: 11}\n",
      "Partition 1 class distribution: {1: 11, 0: 17}\n",
      "Partition 2 class distribution: {1: 52, 0: 5}\n",
      "Partition 3 class distribution: {1: 73, 0: 1}\n",
      "Partition 4 class distribution: {0: 47, 1: 12}\n",
      "Partition 5 class distribution: {0: 13, 1: 20}\n",
      "Partition 6 class distribution: {0: 12, 1: 1}\n",
      "Partition 7 class distribution: {0: 8, 1: 2}\n",
      "Partition 8 class distribution: {1: 4, 0: 3}\n",
      "Partition 9 class distribution: {1: 33, 0: 1}\n",
      "Partition 10 class distribution: {1: 140, 0: 1}\n",
      "Partition 11 class distribution: {1: 123, 0: 44}\n",
      "Partition 12 class distribution: {1: 81, 0: 1}\n",
      "Partition 13 class distribution: {1: 101, 0: 18}\n",
      "Partition 14 class distribution: {1: 795, 0: 5}\n",
      "Partition 15 class distribution: {0: 6, 1: 24}\n",
      "Partition 16 class distribution: {1: 189, 0: 25}\n",
      "Partition 17 class distribution: {0: 5, 1: 9}\n",
      "Partition 18 class distribution: {1: 95, 0: 2}\n",
      "Partition 19 class distribution: {0: 11, 1: 11}\n",
      "Partition 20 class distribution: {1: 1, 0: 1}\n",
      "Partition 21 class distribution: {1: 45, 0: 2}\n",
      "Partition 22 class distribution: {0: 90, 1: 103}\n",
      "Partition 23 class distribution: {1: 15, 0: 3}\n",
      "Partition 24 class distribution: {1: 76, 0: 13}\n",
      "Partition 25 class distribution: {0: 162, 1: 26}\n",
      "Partition 26 class distribution: {0: 1, 1: 3}\n",
      "Partition 27 class distribution: {1: 9, 0: 5}\n",
      "Partition 28 class distribution: {0: 42, 1: 23}\n",
      "Partition 29 class distribution: {0: 57, 1: 76}\n",
      "Partition 30 class distribution: {1: 71, 0: 11}\n",
      "Partition 31 class distribution: {1: 24, 0: 4}\n",
      "Partition 32 class distribution: {1: 43, 0: 2}\n",
      "Partition 33 class distribution: {1: 56, 0: 15}\n",
      "Partition 34 class distribution: {0: 114, 1: 13}\n",
      "Partition 35 class distribution: {0: 78, 1: 3}\n",
      "Partition 36 class distribution: {0: 8, 1: 1}\n",
      "Partition 37 class distribution: {1: 82, 0: 1}\n",
      "Partition 38 class distribution: {1: 3, 0: 1}\n",
      "Partition 39 class distribution: {1: 16, 0: 16}\n",
      "Partition 40 class distribution: {1: 52, 0: 3}\n",
      "Partition 41 class distribution: {1: 52, 0: 70}\n",
      "Partition 42 class distribution: {1: 46, 0: 57}\n",
      "Partition 43 class distribution: {0: 7, 1: 5}\n",
      "Partition 44 class distribution: {1: 27, 0: 1}\n",
      "Partition 45 class distribution: {1: 117, 0: 11}\n",
      "Partition 46 class distribution: {1: 116, 0: 4}\n",
      "Partition 47 class distribution: {0: 46, 1: 7}\n",
      "Partition 48 class distribution: {1: 4, 0: 4}\n",
      "Partition 49 class distribution: {0: 13, 1: 1}\n",
      "Partition 50 class distribution: {1: 37, 0: 4}\n",
      "Partition 51 class distribution: {0: 31, 1: 31}\n",
      "Partition 52 class distribution: {0: 74, 1: 4}\n",
      "Partition 53 class distribution: {1: 21, 0: 6}\n",
      "Partition 54 class distribution: {0: 8, 1: 5}\n",
      "Partition 55 class distribution: {1: 137, 0: 65}\n",
      "Partition 56 class distribution: {1: 139, 0: 18}\n",
      "Partition 57 class distribution: {1: 51, 0: 10}\n",
      "Partition 58 class distribution: {1: 523, 0: 1}\n",
      "Partition 59 class distribution: {0: 46, 1: 1}\n",
      "Number of train samples: 5216, val samples: 0, test samples: 624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/namvq/Data/anaconda3/envs/env_datn/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/media/namvq/Data/anaconda3/envs/env_datn/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG11_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate on cuda\n",
      "evaluate on cuda\n",
      "Round 1/1\n",
      "evaluate on cuda\n",
      "evaluate on cuda\n",
      "Acc_val:  87.5\n",
      "Acc_test:  69.55128205128206\n",
      "Time for round 1:  17.357775926589966\n",
      "accuracies test:  [64.58333333333333, 69.55128205128206]\n",
      "accuracies val:  [50.0, 87.5]\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf  # Thêm OmegaConf\n",
    "\n",
    "# def load_config(config_file):\n",
    "#     \"\"\"Load configuration using OmegaConf (DictConfig)\"\"\"\n",
    "#     # Dùng OmegaConf để load file config.yaml\n",
    "#     config = OmegaConf.load(config_file)\n",
    "#     return config\n",
    "def load_config():\n",
    "    \"\"\"Load configuration using OmegaConf from a dictionary.\"\"\"\n",
    "    config_dict = {\n",
    "        \"num_clients\": 60,\n",
    "        \"num_epochs\": 1,\n",
    "        \"batch_size\": 10,\n",
    "        \"clients_per_round\": 2,\n",
    "        \"fraction_fit\": 0.1,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"num_rounds\": 1,\n",
    "        \"partitioning\": \"label_drl_and_noisy\",\n",
    "        \"dataset_name\": \"chest_xray\",\n",
    "        \"dataset_seed\": 42,\n",
    "        \"alpha\": 0.5,\n",
    "        \"sigma\": 0.1,\n",
    "        \"labels_per_client\": 1,  # only used when partitioning is label quantity\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.00001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"${dataset_name}\",\n",
    "            \"partitioning\": \"${partitioning}\",\n",
    "            \"batch_size\": \"${batch_size}\",  # batch_size = batch_size_ratio * total_local_data_size\n",
    "            \"val_split\": 0.0,\n",
    "            \"seed\": \"${dataset_seed}\",\n",
    "            \"alpha\": \"${alpha}\",\n",
    "            \"sigma\": \"${sigma}\",\n",
    "            \"labels_per_client\": \"${labels_per_client}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Chuyển đổi dictionary thành DictConfig\n",
    "    config = OmegaConf.create(config_dict)\n",
    "\n",
    "    return config\n",
    "\n",
    "def main():\n",
    "    # Parse arguments\n",
    "    # args = parse_args()\n",
    "\n",
    "    # Load configuration file\n",
    "    config = load_config()  # Trả về DictConfig\n",
    "\n",
    "    # Kiểm tra các tham số được thay thế chính xác\n",
    "    print(\"Loaded Config:\")\n",
    "    print(config)\n",
    "\n",
    "    # Load dataset\n",
    "    # trainloaders, valloaders, testloader = load_datasets(config.dataset.name, args.num_clients)\n",
    "    trainloaders, valloaders, testloader = load_datasets(\n",
    "        config=config.dataset,\n",
    "        num_clients=config.num_clients,\n",
    "        val_ratio=config.dataset.val_split,\n",
    "    )\n",
    "\n",
    "    # Train federated model\n",
    "    federated_train(trainloaders, valloaders, testloader, config)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from omegaconf import OmegaConf  # Thêm OmegaConf\n",
    "\n",
    "# # def load_config(config_file):\n",
    "# #     \"\"\"Load configuration using OmegaConf (DictConfig)\"\"\"\n",
    "# #     # Dùng OmegaConf để load file config.yaml\n",
    "# #     config = OmegaConf.load(config_file)\n",
    "# #     return config\n",
    "# def load_config():\n",
    "#     \"\"\"Load configuration using OmegaConf from a dictionary.\"\"\"\n",
    "#     config_dict = {\n",
    "#         \"num_clients\": 4,\n",
    "#         \"num_epochs\": 1,\n",
    "#         \"batch_size\": 10,\n",
    "#         \"clients_per_round\": 2,\n",
    "#         \"fraction_fit\": 0.1,\n",
    "#         \"learning_rate\": 1e-3,\n",
    "#         \"num_rounds\": 1,\n",
    "#         \"partitioning\": \"imbalance_label\",\n",
    "#         \"dataset_name\": \"chest_xray\",\n",
    "#         \"dataset_seed\": 42,\n",
    "#         \"alpha\": 0.5,\n",
    "#         \"sigma\": 0.1,\n",
    "#         \"labels_per_client\": 1,  # only used when partitioning is label quantity\n",
    "#         \"momentum\": 0.9,\n",
    "#         \"weight_decay\": 0.00001,\n",
    "#         \"dataset\": {\n",
    "#             \"name\": \"${dataset_name}\",\n",
    "#             \"partitioning\": \"${partitioning}\",\n",
    "#             \"batch_size\": \"${batch_size}\",  # batch_size = batch_size_ratio * total_local_data_size\n",
    "#             \"val_split\": 0.0,\n",
    "#             \"seed\": \"${dataset_seed}\",\n",
    "#             \"alpha\": \"${alpha}\",\n",
    "#             \"sigma\": \"${sigma}\",\n",
    "#             \"labels_per_client\": \"${labels_per_client}\"\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     # Chuyển đổi dictionary thành DictConfig\n",
    "#     config = OmegaConf.create(config_dict)\n",
    "\n",
    "#     return config\n",
    "\n",
    "# def main():\n",
    "#     # Parse arguments\n",
    "#     # args = parse_args()\n",
    "\n",
    "#     # Load configuration file\n",
    "#     config = load_config()  # Trả về DictConfig\n",
    "\n",
    "#     # Kiểm tra các tham số được thay thế chính xác\n",
    "#     print(\"Loaded Config:\")\n",
    "#     print(config)\n",
    "\n",
    "#     # Load dataset\n",
    "#     # trainloaders, valloaders, testloader = load_datasets(config.dataset.name, args.num_clients)\n",
    "#     trainloaders, valloaders, testloader = load_datasets(\n",
    "#         config=config.dataset,\n",
    "#         num_clients=config.num_clients,\n",
    "#         val_ratio=config.dataset.val_split,\n",
    "#     )\n",
    "\n",
    "#     # Train federated model\n",
    "#     federated_train(trainloaders, valloaders, testloader, config)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_datn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
