{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK_DATASET = \"/media/namvq/Data/chest_xray\"\n",
    "# LINK_DATASET = \"/kaggle/input/chest-xray-pneumonia/chest_xray\"\n",
    "NUM_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKEND:  Agg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Partition the data and create the dataloaders.\"\"\"\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, Grayscale, ToTensor\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Chuyển sang backend không cần GUI\n",
    "\n",
    "print('BACKEND: ', matplotlib.get_backend())\n",
    "\n",
    "\n",
    "#/kaggle/input/chest-xray-pneumonia/chest_xray\n",
    "def get_custom_dataset(data_path: str = LINK_DATASET):\n",
    "    \"\"\"Load custom dataset and apply transformations.\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),  # Kích thước ảnh cho VGG\n",
    "        transforms.RandomAffine(degrees=0, shear=10),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.2, 0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "                             [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((150, 150)),  # Kích thước ảnh cho VGG\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],  # Mean chuẩn của ImageNet\n",
    "                             [0.229, 0.224, 0.225])  # Std chuẩn của ImageNet\n",
    "    ])\n",
    "    trainset = ImageFolder(os.path.join(data_path, 'train'), transform=train_transform)\n",
    "    testset = ImageFolder(os.path.join(data_path, 'test'), transform=test_transform)\n",
    "    return trainset, testset\n",
    "#Lay tap val goc co 16 anh thoi\n",
    "def get_val_dataloader(batch_size: int = 10):\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    valset = ImageFolder(os.path.join(LINK_DATASET, 'val'), transform=val_transform)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    return valloader\n",
    "\n",
    "def prepare_dataset_for_centralized_train(batch_size: int, val_ratio: float = 0.1, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    # Split trainset into trainset and valset\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloader, valloader, testloader\n",
    "\n",
    "\n",
    "def prepare_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1, alpha: float = 100, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate non-IID partitions using Dirichlet distribution.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    \n",
    "    # Split trainset into trainset and valset\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "    \n",
    "    # Generate Dirichlet distribution for each class\n",
    "    class_indices = [np.where(train_labels == i)[0] for i in range(len(np.unique(train_labels)))]\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    \n",
    "    for class_idx in class_indices:\n",
    "        np.random.shuffle(class_idx)\n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, num_partitions))\n",
    "        proportions = (np.cumsum(proportions) * len(class_idx)).astype(int)[:-1]\n",
    "        class_partitions = np.split(class_idx, proportions)\n",
    "        for i in range(num_partitions):\n",
    "            partition_indices[i].extend(class_partitions[i])\n",
    "    \n",
    "    # Create Subsets for each partition\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "    \n",
    "    # Split valset into partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "\n",
    "    valsets = random_split(valset, partition_len_val, torch.Generator().manual_seed(seed))\n",
    "    \n",
    "    # Create DataLoaders for each partition\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Calculate class distribution for each partition in trainloaders\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_partitioned_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1, num_labels_each_party: int = 1, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate partitions where each party has a fixed number of labels.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()  # Load datasets\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels\n",
    "    num_labels = len(np.unique(train_labels))  # Assuming labels are 0 and 1 for binary classification\n",
    "    times = [0 for i in range(num_labels)]\n",
    "    contain = []\n",
    "    #Phan label cho cac client\n",
    "    for i in range(num_partitions):\n",
    "        current = [i%num_labels]\n",
    "        times[i%num_labels] += 1\n",
    "        if num_labels_each_party > 1:\n",
    "            current.append(1-i%num_labels)\n",
    "            times[1-i%num_labels] += 1\n",
    "        contain.append(current)\n",
    "    print(times)\n",
    "    print(contain)\n",
    "    # Create Subsets for each partition\n",
    "\n",
    "    partition_indices = [[] for _ in range(num_partitions)]\n",
    "    for i in range(num_labels):\n",
    "        idx_i = np.where(train_labels == i)[0]  # Get indices of label i in train_labels\n",
    "        idx_i = [trainset.indices[j] for j in idx_i]  # Convert indices to indices in trainset\n",
    "        # #print label of idx_i\n",
    "        # print(\"Label of idx: \", i)\n",
    "        # for j in range(len(idx_i)):\n",
    "        #     idx_in_dataset = trainset.indices[idx_i[j]]\n",
    "        #     print(trainset.dataset.targets[idx_in_dataset])\n",
    "        np.random.shuffle(idx_i)\n",
    "        split = np.array_split(idx_i, times[i])\n",
    "        ids = 0\n",
    "        for j in range(num_partitions):\n",
    "            if i in contain[j]:\n",
    "                partition_indices[j].extend(split[ids])\n",
    "                ids += 1\n",
    "    \n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    # #print label of client 0\n",
    "    # print(\"Client 0\")\n",
    "    # for i in range(len(trainsets[0])):\n",
    "    #     print(trainsets[0][i][1])\n",
    "\n",
    "    # Split valset into partitions\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Create DataLoaders for each partition\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Calculate class distribution for each partition in trainloaders\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "def prepare_imbalance_label_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 0.5, seed: int = 42):\n",
    "    \"\"\"Load custom dataset and generate partitions where each party has a fixed number of labels.\"\"\"\n",
    "    trainset, testset = get_custom_dataset()  # Load datasets\n",
    "\n",
    "    # Split the trainset into trainset and valset based on the validation ratio\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Get labels for the entire trainset\n",
    "    train_labels = np.array([trainset.dataset.targets[i] for i in trainset.indices])\n",
    "\n",
    "    # Define partitions: each party has k labels\n",
    "    num_labels = len(np.unique(train_labels))  # Assuming labels are 0 and 1 for binary classification\n",
    "    min_size = 0\n",
    "    min_require_size = 2\n",
    "\n",
    "    N = len(trainset)\n",
    "\n",
    "\n",
    "    while(min_size < min_require_size):\n",
    "        partition_indices = [[] for _ in range(num_partitions)]\n",
    "        for label in range(num_labels):\n",
    "            idx_label = np.where(train_labels == label)[0]\n",
    "            idx_label = [trainset.indices[j] for j in idx_label]\n",
    "            np.random.shuffle(idx_label)\n",
    "\n",
    "            proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "            # proportions = np.array( [p * len(idx_j) < N/num_partitions] for p, idx_j in zip(proportions, partition_indices))\n",
    "            proportions = np.array([p if p * len(idx_j) < N / num_partitions else 0 for p, idx_j in zip(proportions, partition_indices)])\n",
    "\n",
    "            proportions = proportions / np.sum(proportions)\n",
    "            proportions = (np.cumsum(proportions) * len(idx_label)).astype(int)[:-1]\n",
    "\n",
    "            partition_indices = [idx_j + idx.tolist() for idx_j, idx in zip(partition_indices, np.split(idx_label, proportions))]\n",
    "            min_size = min([len(idx_j) for idx_j in partition_indices])\n",
    "        \n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    # Plot class distribution\n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "\n",
    "def apply_gaussian_noise(tensor, std_dev):\n",
    "    noise = torch.randn_like(tensor) * std_dev\n",
    "    return tensor + noise\n",
    "\n",
    "# Hàm đảo ngược chuẩn hóa\n",
    "def unnormalize_image(image_tensor, mean, std):\n",
    "    # Đảo ngược Normalize: (image * std) + mean\n",
    "    for t, m, s in zip(image_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Thực hiện từng kênh\n",
    "    return image_tensor\n",
    "\n",
    "# Hàm hiển thị ảnh từ một tensor\n",
    "def display_image(image_tensor, mean, std):\n",
    "    # Đảo ngược chuẩn hóa\n",
    "    image_tensor = unnormalize_image(image_tensor, mean, std)\n",
    "    # Chuyển tensor thành NumPy array và điều chỉnh thứ tự kênh màu (CHW -> HWC)\n",
    "    image_numpy = image_tensor.permute(1, 2, 0).numpy()\n",
    "    # Cắt giá trị ảnh về phạm vi [0, 1] để hiển thị đúng\n",
    "    image_numpy = image_numpy.clip(0, 1)\n",
    "    # Trả về ảnh NumPy\n",
    "    return image_numpy\n",
    "\n",
    "def prepare_noise_based_imbalance(num_partitions: int, batch_size: int, val_ratio: float = 0.1, sigma: float = 0.05, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Chia du lieu ngau nhien va deu cho cac ben, sau do them noise vao cac ben\n",
    "    moi ben i co noise khac nhau Gauss(0, sigma*i/N)\n",
    "    \"\"\"\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    indices = trainset.indices\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    partition_indices = np.array_split(indices, num_partitions)\n",
    "\n",
    "    train_partitions = []\n",
    "\n",
    "    for i, part_indices in enumerate(partition_indices):\n",
    "        partition_std_dev = sigma * (i + 1) / num_partitions\n",
    "        partition_set = Subset(trainset.dataset, part_indices)\n",
    "        \n",
    "        noisy_samples = [apply_gaussian_noise(sample[0], partition_std_dev) for sample in partition_set]\n",
    "        noisy_dataset = [(noisy_samples[j], trainset.dataset[part_indices[j]][1]) for j in range(len(part_indices))]\n",
    "        # train_partitions.append((noisy_samples, [sample[1] for sample in partition_set]))\n",
    "        train_partitions.append(noisy_dataset)\n",
    "    trainloaders = [DataLoader(train_partitions[i], batch_size=batch_size, shuffle=True, num_workers=4) for i in range(num_partitions)]\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "####\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    #Lưu ảnh nhiễu vào running_outputs\n",
    "    # Mean và std từ Normalize\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # Tạo thư mục lưu ảnh nếu chưa tồn tại\n",
    "    output_dir = \"running_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Khởi tạo một lưới 10x6 để hiển thị ảnh\n",
    "    fig, axes = plt.subplots(10, 6, figsize=(15, 25))\n",
    "\n",
    "    # Duyệt qua 60 trainloaders và hiển thị ảnh đầu tiên\n",
    "    for i, trainloader in enumerate(trainloaders[:num_partitions]):\n",
    "        # Lấy ảnh đầu tiên từ trainloader\n",
    "        image_tensor = trainloader.dataset[0][0].clone()  # Clone để tránh thay đổi dữ liệu gốc\n",
    "        \n",
    "        # Tìm vị trí hàng, cột trong lưới\n",
    "        row, col = divmod(i, 6)\n",
    "        plt.sca(axes[row, col])  # Đặt trục hiện tại là vị trí hàng, cột trong lưới\n",
    "        \n",
    "        # Hiển thị ảnh\n",
    "        image_numpy = display_image(image_tensor, mean, std)\n",
    "        axes[row, col].imshow(image_numpy)\n",
    "        axes[row, col].axis('off')\n",
    "    plt.title(f\"Noise image with sigma from {sigma * 1 / num_partitions} to {sigma}\")\n",
    "    # Điều chỉnh layout để không bị chồng lấn\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Lưu ảnh thay vì hiển thị\n",
    "    output_path = os.path.join(output_dir, \"image_noise.png\")\n",
    "    plt.savefig(output_path, dpi=300)  # Lưu ảnh với chất lượng cao\n",
    "\n",
    "    plt.close()  # Đóng figure\n",
    "\n",
    "    print(f\"Ảnh đã được lưu tại {output_path}\")\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "###\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "def prepare_quantity_skew_dirichlet(num_partitions: int, batch_size: int, val_ratio: float = 0.1, beta: float = 10, seed: int = 42):\n",
    "    trainset, testset = get_custom_dataset()\n",
    "    num_train = int((1 - val_ratio) * len(trainset))\n",
    "    num_val = len(trainset) - num_train\n",
    "    trainset, valset = random_split(trainset, [num_train, num_val], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    all_indices = trainset.indices\n",
    "\n",
    "    min_size = 0\n",
    "    while min_size < 1:\n",
    "        proportions = np.random.dirichlet(np.repeat(beta, num_partitions))\n",
    "        proportions = (np.cumsum(proportions) * len(all_indices)).astype(int)[:-1]\n",
    "\n",
    "        partition_indices = np.split(all_indices, proportions)\n",
    "\n",
    "        min_size = min([len(partition) for partition in partition_indices])\n",
    "        print('Partition sizes:', [len(partition) for partition in partition_indices])\n",
    "        print('Min partition size:', min_size)\n",
    "\n",
    "    trainsets = [Subset(trainset.dataset, indices) for indices in partition_indices]\n",
    "\n",
    "    partition_len_val = [len(valset) // num_partitions] * num_partitions\n",
    "    for i in range(len(valset) % num_partitions):\n",
    "        partition_len_val[i] += 1\n",
    "    \n",
    "    valsets = random_split(valset, partition_len_val, generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    trainloaders = [DataLoader(ts, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS) for ts in trainsets]\n",
    "    valloaders = [DataLoader(vs, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS) for vs in valsets]\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    class_distributions = []\n",
    "    for i, trainloader in enumerate(trainloaders):\n",
    "        class_counts = Counter()\n",
    "        for _, labels in trainloader:\n",
    "            class_counts.update(labels.numpy())\n",
    "        class_distributions.append(class_counts)\n",
    "        print(f'Partition {i} class distribution: {dict(class_counts)}')\n",
    "    \n",
    "    partitions = range(num_partitions)\n",
    "    class_0_counts = [class_distributions[i][0] for i in partitions]\n",
    "    class_1_counts = [class_distributions[i][1] for i in partitions]\n",
    "\n",
    "    bar_width = 0.5\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(partitions, class_0_counts, bar_width, label='Class 0', color='blue')\n",
    "    plt.bar(partitions, class_1_counts, bar_width, bottom=class_0_counts, label='Class 1', color='red')\n",
    "    plt.xlabel('Partition')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Class Distribution in Each Partition')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # plt.show()\n",
    "    #  Lưu đồ thị vào thư mục running_outputs với tên data_partition\n",
    "    output_dir = 'running_outputs'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir, 'data_partition.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f'Number of train samples: {len(trainset)}, val samples: {len(valset)}, test samples: {len(testloader.dataset)}')\n",
    "\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "def load_datasets(\n",
    "    config: DictConfig,\n",
    "    num_clients: int,\n",
    "    val_ratio: float = 0.1,\n",
    "    seed: Optional[int] = 42,\n",
    ") -> Tuple[List[DataLoader], List[DataLoader], DataLoader]:\n",
    "    \"\"\"Create the dataloaders to be fed into the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config: DictConfig\n",
    "        Parameterises the dataset partitioning process\n",
    "    num_clients : int\n",
    "        The number of clients that hold a part of the data\n",
    "    val_ratio : float, optional\n",
    "        The ratio of training data that will be used for validation (between 0 and 1),\n",
    "        by default 0.1\n",
    "    seed : int, optional\n",
    "        Used to set a fix seed to replicate experiments, by default 42\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[DataLoader, DataLoader, DataLoader]\n",
    "        The DataLoaders for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    print(f\"Dataset partitioning config: {config}\")\n",
    "    batch_size = -1\n",
    "    print('config:' , config)\n",
    "    if \"batch_size\" in config:\n",
    "        batch_size = config.batch_size\n",
    "    elif \"batch_size_ratio\" in config:\n",
    "        batch_size_ratio = config.batch_size_ratio\n",
    "    else:\n",
    "        raise ValueError\n",
    "    partitioning = \"\"\n",
    "    \n",
    "    if \"partitioning\" in config:\n",
    "        partitioning = config.partitioning\n",
    "\n",
    "    # partition the data\n",
    "    if partitioning == \"imbalance_label\":\n",
    "        return prepare_partitioned_dataset(num_clients, batch_size, val_ratio, config.labels_per_client, config.seed)\n",
    "\n",
    "    if partitioning == \"imbalance_label_dirichlet\":\n",
    "        return prepare_imbalance_label_dirichlet(num_clients, batch_size, val_ratio, config.alpha, config.seed)\n",
    "\n",
    "    if partitioning == \"noise_based_imbalance\":\n",
    "        return prepare_noise_based_imbalance(num_clients, batch_size, val_ratio, config.sigma, config.seed)\n",
    "\n",
    "    if partitioning == \"quantity_skew_dirichlet\":\n",
    "        return prepare_quantity_skew_dirichlet(num_clients, batch_size, val_ratio, config.alpha, config.seed)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        # self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)  # Output corresponds to num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class VGG11Model(nn.Module):\n",
    "    # Implement VGG11 model for transfer learning\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = models.vgg11(pretrained=True)\n",
    "        \n",
    "        # Freeze the convolutional base\n",
    "        # for param in self.model.features.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        # Replace avgpool with AdaptiveAvgPool2d\n",
    "        self.model.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Replace the classifier with a new one\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)  # Output corresponds to num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim \n",
    "import copy\n",
    "import random \n",
    "import numpy as np\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def reset_model_to_zero(model):\n",
    "    for param in model.parameters():\n",
    "        param.data.fill_(0.0)\n",
    "\n",
    "def federated_train(trainloaders, valloaders, testloader, config):\n",
    "    model = ResNet18(num_classes=2)\n",
    "    # model = VGG11Model(num_classes=2)\n",
    "    nets = {net_i: copy.deepcopy(model) for net_i in range(len(trainloaders))}\n",
    "    global_model = copy.deepcopy(model)  # Bản sao mô hình toàn cục\n",
    "    c_nets = {net_i: copy.deepcopy(model) for net_i in range(len(trainloaders))}  # Bản sao mô hình trên từng client\n",
    "    c_global = copy.deepcopy(model)\n",
    "\n",
    "    # c_global_para = c_global.state_dict()\n",
    "    # for net_id, net in c_nets.items():\n",
    "    #     net.load_state_dict(c_global_para)\n",
    "    for c_net in c_nets.values():\n",
    "        reset_model_to_zero(c_net)\n",
    "    reset_model_to_zero(c_global)\n",
    "\n",
    "    valloader_goc = get_val_dataloader()\n",
    "    num_rounds = config.num_rounds  # Số vòng huấn luyện\n",
    "    accs_test = []\n",
    "    accs_val = []\n",
    "    accs_test.append(evaluate(global_model, testloader))\n",
    "    accs_val.append(evaluate(global_model, valloader_goc))\n",
    "    for round_num in range(num_rounds):\n",
    "        print(f\"Round {round_num + 1}/{num_rounds}\")\n",
    "        start = time.time()\n",
    "        global_para = global_model.state_dict()\n",
    "\n",
    "        # Chọn các client tham gia vào mỗi round\n",
    "        selected_clients = select_clients(trainloaders, config.clients_per_round)\n",
    "        \n",
    "        # Huấn luyện trên các client đã chọn\n",
    "        for client in selected_clients:\n",
    "            nets[client].load_state_dict(global_para)\n",
    "        local_train_net_scaffold(nets, selected_clients, global_model, c_nets, c_global, config, trainloaders, device=DEVICE)\n",
    "\n",
    "        total_data_points = sum([len(trainloaders[client].dataset) for client in selected_clients])\n",
    "        freqs = [len(trainloaders[client].dataset) / total_data_points for client in selected_clients]\n",
    "\n",
    "        for idx in range(len(selected_clients)):\n",
    "            net_para = nets[selected_clients[idx]].cpu().state_dict()\n",
    "            if idx == 0:\n",
    "                for key in net_para:\n",
    "                    global_para[key] = net_para[key] * freqs[idx]\n",
    "            else:\n",
    "                for key in net_para:\n",
    "                    global_para[key] += net_para[key] * freqs[idx]\n",
    "        global_model.load_state_dict(global_para)\n",
    "        global_model.to('cpu')\n",
    "        acc_test = evaluate(global_model, testloader)        \n",
    "        acc_val = evaluate(global_model, valloader_goc)\n",
    "\n",
    "        accs_test.append(acc_test)\n",
    "        accs_val.append(acc_val)\n",
    "        if round_num >= 0:\n",
    "            if acc_val > 80.0:\n",
    "                config.learning_rate = 1e-8\n",
    "                print(f\"Accuracy > 80%, decreasing learning rate to {config.learning_rate}\")\n",
    "            elif acc_val > 70.0:\n",
    "                config.learning_rate = 1e-7\n",
    "                print(f\"Accuracy > 70%, decreasing learning rate to {config.learning_rate}\")\n",
    "            elif acc_val > 60.0:\n",
    "                config.learning_rate = 1e-6\n",
    "                print(f\"Accuracy > 60%, decreasing learning rate to {config.learning_rate}\")\n",
    "            elif acc_val > 50.0:\n",
    "                config.learning_rate = 1e-5\n",
    "                print(f\"Accuracy > 50%, decreasing learning rate to {config.learning_rate}\")\n",
    "            else :\n",
    "                config.learning_rate = 1e-4\n",
    "                print(f\"Accuracy <= 50%, increasing learning rate to {config.learning_rate}\")\n",
    "\n",
    "        end = time.time()\n",
    "        print(f'Time for round {round_num + 1}: ', end-start)\n",
    "    # plot_accuracy(accs)\n",
    "        # plot_accuracy(accs)\n",
    "    print('accuracies test: ', accs_test)\n",
    "    print('accuracies val: ', accs_val)\n",
    "    plt.plot(range(0, num_rounds + 1), accs_test, marker='o', label='Accuracy_test')\n",
    "    plt.plot(range(0, num_rounds + 1), accs_val, marker='x', label='Accuracy_val')\n",
    "    plt.xlabel('Round')\n",
    "    plt.xticks(range(0, num_rounds + 1, 10))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('WOW WOW WOW')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    idx_file = 1\n",
    "    new_file_name = f'running_outputs/accuracy_summary_{idx_file}.png'\n",
    "    while os.path.exists(f'running_outputs/accuracy_summary_{idx_file}.png'):\n",
    "        idx_file += 1\n",
    "        new_file_name = f'running_outputs/accuracy_summary_{idx_file}.png'\n",
    "    \n",
    "    plt.savefig(new_file_name)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def local_train_net_scaffold(nets, selected_clients, global_model, c_nets, c_global, config, trainloaders, device='cpu'):\n",
    "    total_delta = copy.deepcopy(global_model.state_dict())\n",
    "    for key in total_delta:\n",
    "        total_delta[key] = 0.0\n",
    "    c_global.to(device)\n",
    "    global_model.to(device)\n",
    "    for net_id in selected_clients:\n",
    "        net = nets[net_id]\n",
    "        net.to(device)\n",
    "\n",
    "        c_nets[net_id].to(device)\n",
    "\n",
    "        c_delta_para = train_net_scaffold(net, global_model, c_nets[net_id], c_global, trainloaders[net_id], config, device=device)\n",
    "        c_nets[net_id].to('cpu')\n",
    "        for key in total_delta:\n",
    "            total_delta[key] += c_delta_para[key]\n",
    "        \n",
    "    for key in total_delta:\n",
    "        # total_delta[key] /= len(selected_clients) ### ???\n",
    "        total_delta[key] /= config.num_clients\n",
    "    c_global_para = c_global.state_dict()\n",
    "    for key in c_global_para:\n",
    "        if c_global_para[key].type() == 'torch.LongTensor':\n",
    "            c_global_para[key] += total_delta[key].type(torch.LongTensor)\n",
    "        elif c_global_para[key].type() == 'torch.cuda.LongTensor':\n",
    "            c_global_para[key] += total_delta[key].type(torch.cuda.LongTensor)\n",
    "        else:\n",
    "            #print(c_global_para[key].type())\n",
    "            c_global_para[key] += total_delta[key]\n",
    "    c_global.load_state_dict(c_global_para)\n",
    "\n",
    "    # nets_list = list(nets.values())\n",
    "    # return nets_list\n",
    "def train_net_scaffold(net, global_model, c_local, c_global, trainloader, config, device):\n",
    "    optimizer = optim.SGD(\n",
    "        filter(lambda p: p.requires_grad, net.parameters()),\n",
    "        lr=config.learning_rate,\n",
    "        momentum=config.momentum\n",
    "    )\n",
    "    critierion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    c_local.to(device)\n",
    "    c_global.to(device)\n",
    "    global_model.to(device)\n",
    "\n",
    "    c_global_para = c_global.state_dict()\n",
    "    c_local_para = c_local.state_dict()\n",
    "    cnt = 0\n",
    "    for _ in range(config.num_epochs):\n",
    "        for data, target in trainloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = critierion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            net_para = net.state_dict()\n",
    "            for key in net_para:\n",
    "                net_para[key] = net_para[key] - config.learning_rate * (c_global_para[key] - c_local_para[key])\n",
    "            net.load_state_dict(net_para)\n",
    "            cnt += 1\n",
    "    \n",
    "    c_new_para = c_local.state_dict()\n",
    "    c_delta_para = copy.deepcopy(c_local.state_dict())\n",
    "    global_model_para = global_model.state_dict()\n",
    "    net_para = net.state_dict()\n",
    "    for key in net_para:\n",
    "        c_new_para[key] = c_new_para[key] - c_global_para[key] + (global_model_para[key] - net_para[key]) / (cnt * config.learning_rate)\n",
    "        c_delta_para[key] = c_new_para[key] - c_local_para[key]\n",
    "    c_local.load_state_dict(c_new_para)\n",
    "\n",
    "    net.to('cpu')\n",
    "    return c_delta_para\n",
    "\n",
    "            \n",
    "        \n",
    "def plot_accuracy(accs):\n",
    "    print('accuracies: ', accs)\n",
    "    num_rounds = len(accs)-1\n",
    "    plt.plot(range(0, num_rounds + 1), accs, marker='o', label='Accuracy')\n",
    "    plt.xlabel('Round')\n",
    "    plt.xticks(range(0, num_rounds + 1))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Scaffold on ResNet18 over Rounds')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig('running_outputs/accuracy_summary.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def select_clients(trainloaders, clients_per_round):\n",
    "    \"\"\"Chọn ngẫu nhiên một số client tham gia huấn luyện trong mỗi round.\"\"\"\n",
    "    # Số lượng client có sẵn\n",
    "    total_clients = len(trainloaders)\n",
    "    # Chọn ngẫu nhiên một số client\n",
    "    selected_clients = random.sample(range(total_clients), clients_per_round)\n",
    "    return selected_clients\n",
    "\n",
    "\n",
    "def evaluate(model, testloader):\n",
    "    \"\"\"Đánh giá mô hình trên tập kiểm tra.\"\"\"\n",
    "    model.eval()  # Chuyển sang chế độ đánh giá\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đường dẫn thư mục\n",
    "directory = 'running_outputs/'\n",
    "\n",
    "# Xóa tất cả tệp trong thư mục bằng một lệnh duy nhất\n",
    "for file in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Config:\n",
      "{'num_clients': 4, 'num_epochs': 1, 'batch_size': 10, 'clients_per_round': 2, 'fraction_fit': 0.1, 'learning_rate': 0.001, 'num_rounds': 1, 'partitioning': 'imbalance_label', 'dataset_name': 'chest_xray', 'dataset_seed': 42, 'alpha': 0.5, 'sigma': 0.1, 'labels_per_client': 1, 'momentum': 0.9, 'weight_decay': 1e-05, 'dataset': {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}}\n",
      "Dataset partitioning config: {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}\n",
      "config: {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}\n",
      "[2, 2]\n",
      "[[0], [1], [0], [1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0 class distribution: {0: 26}\n",
      "Partition 1 class distribution: {1: 70}\n",
      "Partition 2 class distribution: {0: 25}\n",
      "Partition 3 class distribution: {1: 70}\n",
      "Number of train samples: 191, val samples: 0, test samples: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/media/namvq/Data/anaconda3/envs/env1/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 53.49%\n",
      "Test Accuracy: 50.00%\n",
      "Round 1/1\n",
      "Test Accuracy: 48.06%\n",
      "Test Accuracy: 50.00%\n",
      "Accuracy <= 50%, increasing learning rate to 0.0001\n",
      "Time for round 1:  7.256776809692383\n",
      "accuracies test:  [53.48837209302326, 48.06201550387597]\n",
      "accuracies val:  [50.0, 50.0]\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf  # Thêm OmegaConf\n",
    "\n",
    "# def load_config(config_file):\n",
    "#     \"\"\"Load configuration using OmegaConf (DictConfig)\"\"\"\n",
    "#     # Dùng OmegaConf để load file config.yaml\n",
    "#     config = OmegaConf.load(config_file)\n",
    "#     return config\n",
    "def load_config():\n",
    "    \"\"\"Load configuration using OmegaConf from a dictionary.\"\"\"\n",
    "    config_dict = {\n",
    "        \"num_clients\": 4,\n",
    "        \"num_epochs\": 1,\n",
    "        \"batch_size\": 10,\n",
    "        \"clients_per_round\": 2,\n",
    "        \"fraction_fit\": 0.1,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"num_rounds\": 1,\n",
    "        \"partitioning\": \"imbalance_label\",\n",
    "        \"dataset_name\": \"chest_xray\",\n",
    "        \"dataset_seed\": 42,\n",
    "        \"alpha\": 0.5,\n",
    "        \"sigma\": 0.1,\n",
    "        \"labels_per_client\": 1,  # only used when partitioning is label quantity\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.00001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"${dataset_name}\",\n",
    "            \"partitioning\": \"${partitioning}\",\n",
    "            \"batch_size\": \"${batch_size}\",  # batch_size = batch_size_ratio * total_local_data_size\n",
    "            \"val_split\": 0.0,\n",
    "            \"seed\": \"${dataset_seed}\",\n",
    "            \"alpha\": \"${alpha}\",\n",
    "            \"sigma\": \"${sigma}\",\n",
    "            \"labels_per_client\": \"${labels_per_client}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Chuyển đổi dictionary thành DictConfig\n",
    "    config = OmegaConf.create(config_dict)\n",
    "\n",
    "    return config\n",
    "\n",
    "def main():\n",
    "    # Parse arguments\n",
    "    # args = parse_args()\n",
    "\n",
    "    # Load configuration file\n",
    "    config = load_config()  # Trả về DictConfig\n",
    "\n",
    "    # Kiểm tra các tham số được thay thế chính xác\n",
    "    print(\"Loaded Config:\")\n",
    "    print(config)\n",
    "\n",
    "    # Load dataset\n",
    "    # trainloaders, valloaders, testloader = load_datasets(config.dataset.name, args.num_clients)\n",
    "    trainloaders, valloaders, testloader = load_datasets(\n",
    "        config=config.dataset,\n",
    "        num_clients=config.num_clients,\n",
    "        val_ratio=config.dataset.val_split,\n",
    "    )\n",
    "\n",
    "    # Train federated model\n",
    "    federated_train(trainloaders, valloaders, testloader, config)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Config:\n",
      "{'num_clients': 4, 'num_epochs': 1, 'batch_size': 10, 'clients_per_round': 2, 'fraction_fit': 0.1, 'learning_rate': 0.001, 'num_rounds': 1, 'partitioning': 'imbalance_label', 'dataset_name': 'chest_xray', 'dataset_seed': 42, 'alpha': 0.5, 'sigma': 0.1, 'labels_per_client': 1, 'momentum': 0.9, 'weight_decay': 1e-05, 'dataset': {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}}\n",
      "Dataset partitioning config: {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}\n",
      "config: {'name': '${dataset_name}', 'partitioning': '${partitioning}', 'batch_size': '${batch_size}', 'val_split': 0.0, 'seed': '${dataset_seed}', 'alpha': '${alpha}', 'sigma': '${sigma}', 'labels_per_client': '${labels_per_client}'}\n",
      "[2, 2]\n",
      "[[0], [1], [0], [1]]\n",
      "Partition 0 class distribution: {0: 26}\n",
      "Partition 1 class distribution: {1: 70}\n",
      "Partition 2 class distribution: {0: 25}\n",
      "Partition 3 class distribution: {1: 70}\n",
      "Number of train samples: 191, val samples: 0, test samples: 129\n",
      "Test Accuracy: 60.47%\n",
      "Test Accuracy: 50.00%\n",
      "Round 1/1\n",
      "Test Accuracy: 46.51%\n",
      "Test Accuracy: 50.00%\n",
      "Accuracy <= 50%, increasing learning rate to 0.0001\n",
      "Time for round 1:  5.253810167312622\n",
      "accuracies test:  [60.46511627906977, 46.51162790697674]\n",
      "accuracies val:  [50.0, 50.0]\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf  # Thêm OmegaConf\n",
    "\n",
    "# def load_config(config_file):\n",
    "#     \"\"\"Load configuration using OmegaConf (DictConfig)\"\"\"\n",
    "#     # Dùng OmegaConf để load file config.yaml\n",
    "#     config = OmegaConf.load(config_file)\n",
    "#     return config\n",
    "def load_config():\n",
    "    \"\"\"Load configuration using OmegaConf from a dictionary.\"\"\"\n",
    "    config_dict = {\n",
    "        \"num_clients\": 4,\n",
    "        \"num_epochs\": 1,\n",
    "        \"batch_size\": 10,\n",
    "        \"clients_per_round\": 2,\n",
    "        \"fraction_fit\": 0.1,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"num_rounds\": 1,\n",
    "        \"partitioning\": \"imbalance_label\",\n",
    "        \"dataset_name\": \"chest_xray\",\n",
    "        \"dataset_seed\": 42,\n",
    "        \"alpha\": 0.5,\n",
    "        \"sigma\": 0.1,\n",
    "        \"labels_per_client\": 1,  # only used when partitioning is label quantity\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 0.00001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"${dataset_name}\",\n",
    "            \"partitioning\": \"${partitioning}\",\n",
    "            \"batch_size\": \"${batch_size}\",  # batch_size = batch_size_ratio * total_local_data_size\n",
    "            \"val_split\": 0.0,\n",
    "            \"seed\": \"${dataset_seed}\",\n",
    "            \"alpha\": \"${alpha}\",\n",
    "            \"sigma\": \"${sigma}\",\n",
    "            \"labels_per_client\": \"${labels_per_client}\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Chuyển đổi dictionary thành DictConfig\n",
    "    config = OmegaConf.create(config_dict)\n",
    "\n",
    "    return config\n",
    "\n",
    "def main():\n",
    "    # Parse arguments\n",
    "    # args = parse_args()\n",
    "\n",
    "    # Load configuration file\n",
    "    config = load_config()  # Trả về DictConfig\n",
    "\n",
    "    # Kiểm tra các tham số được thay thế chính xác\n",
    "    print(\"Loaded Config:\")\n",
    "    print(config)\n",
    "\n",
    "    # Load dataset\n",
    "    # trainloaders, valloaders, testloader = load_datasets(config.dataset.name, args.num_clients)\n",
    "    trainloaders, valloaders, testloader = load_datasets(\n",
    "        config=config.dataset,\n",
    "        num_clients=config.num_clients,\n",
    "        val_ratio=config.dataset.val_split,\n",
    "    )\n",
    "\n",
    "    # Train federated model\n",
    "    federated_train(trainloaders, valloaders, testloader, config)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
